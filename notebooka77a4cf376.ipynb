{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pandas'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"]}],"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize \n","df = pd.read_csv('/kaggle/input/suicide-watch/Suicide_Detection.csv', encoding=\"ISO-8859-1\")\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["suicide = df[df['class']=='suicide']\n","non_suicide = df[df['class']== 'non-suicide']\n","suicide = suicide.head(25000)\n","non_suicide = non_suicide.head(25000)\n","df = pd.concat([suicide,non_suicide])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.isnull().sum()"]},{"cell_type":"markdown","metadata":{},"source":["# **PRE-PROCESSING**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install tqdm\n","!pip install text_hammer\n","import text_hammer as th\n","!pip install --force-reinstall --no-deps beautifulsoup4==4.12.2\n","\n","\n","from tqdm import tqdm\n","import pandas as pd\n","import text_hammer as th\n","\n","tqdm.pandas()\n","\n","def text_preprocessing(df, col_name):\n","    column = col_name\n","    df[column] = df[column].progress_apply(lambda x: str(x).lower())\n","    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n","    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n","    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n","    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n","    return df\n","\n","# Apply text preprocessing\n","df = text_preprocessing(df, 'text')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_preprocess = df.copy()\n","posts = df_preprocess.text.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def removeWordWithChar(text, char_list):\n","    #Remove words in a text that contains a char from the list.\n","    text = text.split()\n","    res = [ele for ele in text if all(ch not in ele for ch in char_list)]\n","    res = ' '.join(res)\n","    return res\n","\n","char_list = ['@', '#', 'http', 'www', '/', '[]']\n","\n","removeWordWithChar(posts[1], char_list)\n","\n","posts_cleaned = []\n","\n","for p in posts:\n","    posts_cleaned.append(removeWordWithChar(p, char_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["posts_cleaned[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(posts_cleaned)"]},{"cell_type":"markdown","metadata":{},"source":["## **Tokenization**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def tokenize(texts):\n","    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n","\n","    texts_tokens = []\n","    for i, val in enumerate(texts):\n","        text_tokens = tokenizer.tokenize(val.lower())\n","\n","        for i in range(len(text_tokens) - 1, -1, -1):\n","            if len(text_tokens[i]) < 4:\n","                del (text_tokens[i])\n","\n","        texts_tokens.append(text_tokens)\n","\n","    return texts_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["posts_tokens = tokenize(posts_cleaned)\n","posts_tokens[:1]"]},{"cell_type":"markdown","metadata":{},"source":["## **StopWords**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","def removeSW(texts_tokens):\n","    stopWords = set(stopwords.words('english'))\n","    texts_filtered = []\n","\n","    for i, val in enumerate(texts_tokens):\n","        text_filtered = []\n","        for w in val:\n","            if w not in stopWords:\n","                text_filtered.append(w)\n","        texts_filtered.append(text_filtered)\n","\n","    return texts_filtered"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","posts_filtered = removeSW(posts_tokens)\n","posts_filtered[:1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from collections import Counter\n","from collections import Counter\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Count word frequencies\n","word_counts = Counter(word for post in posts_filtered for word in post)\n","\n","# Convert to DataFrame\n","word_freq = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n","\n","# Sort by count in descending order\n","word_freq = word_freq.sort_values(by='count', ascending=False)\n","\n","# Plot the top 50 words\n","plt.figure(figsize=(15, 20))\n","sns.barplot(x='count', y='word', data=word_freq.iloc[:50])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from wordcloud import WordCloud  # Import WordCloud\n","\n","# Assuming you have defined feature_names\n","feature_names = word_freq['word'].values\n","\n","# Create WordCloud object\n","wc = WordCloud(max_words=300)\n","\n","# Generate WordCloud\n","wc.generate(' '.join(word for word in feature_names[500:3500]))\n","\n","# Plot the WordCloud\n","plt.figure(figsize=(17, 12))\n","plt.axis('off')\n","plt.imshow(wc)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{},"source":["## **Lemmatization**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def lemma(texts_filtered):\n","    wordnet_lemmatizer = WordNetLemmatizer()\n","    texts_lem = []\n","\n","    for i, val in enumerate(texts_filtered):\n","        text_lem = []\n","        for word in val:\n","            text_lem.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n","        texts_lem.append(text_lem)\n","\n","    return texts_lem\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import nltk\n","import subprocess\n","\n","# Download and unzip wordnet\n","try:\n","    nltk.data.find('wordnet.zip')\n","except:\n","    nltk.download('wordnet', download_dir='/kaggle/working/')\n","    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","    subprocess.run(command.split())\n","    nltk.data.path.append('/kaggle/working/')\n","\n","from nltk.corpus import wordnet\n","\n","posts_lem = lemma(posts_filtered)\n","\n","posts_lem[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["posts_ready = []\n","for posts in posts_lem:\n","    string = ' '\n","    string = string.join(posts)\n","    posts_ready.append(string)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["len(posts_ready)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# original DataFrame and posts_ready list of preprocessed text\n","df_preprocess['original_text'] = df['text']\n","df_preprocess['preprocessed_text'] = posts_ready\n","\n","# Display the resulting DataFrame\n","df_preprocess[['original_text', 'preprocessed_text', 'class']].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data = [{'text': text, 'class': label} for text, label in zip(posts_ready, df['class'])]\n","\n","# Convert the list of dictionaries to a DataFrame\n","df_ready = pd.DataFrame(data)\n","\n","# Separate X and y\n","X = df_ready['text']\n","y = df_ready['class']\n","\n","df_ready"]},{"cell_type":"markdown","metadata":{},"source":["## **Text to Numeric**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer\n"," # this means 10000 unique words can be taken \n","tokenizer=Tokenizer(num_words= 25000,lower=True)\n","tokenizer.fit_on_texts(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","X = tokenizer.texts_to_sequences(X) # this converts texts into some numeric sequences \n","X = pad_sequences(X,maxlen=150,padding='post') # this makes the length of all numeric sequences equal \n","X[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X.shape"]},{"cell_type":"markdown","metadata":{},"source":["## **Word2Vec**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# pip install gensim\n","import gensim.downloader as api\n","glove_gensim  = api.load('glove-wiki-gigaword-100') #100 dimension"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# from gensim.models import Word2Vec\n","# model = Word2Vec(sentences,min_count = 2)\n","# words = model.wv.index_to_key\n","from gensim.models import KeyedVectors\n","\n","#glove_model = KeyedVectors.load_word2vec_format('/kaggle/input/glove42b300dtxt/glove.42B.300d.txt', binary=False)\n","\n","vector_size = 100\n","num_words = 25000\n","gensim_weight_matrix = np.zeros((num_words ,vector_size))\n","gensim_weight_matrix.shape\n","for word, index in tokenizer.word_index.items():\n","    if index < num_words: # since index starts with zero \n","        if word in glove_gensim.index_to_key:\n","            gensim_weight_matrix[index] = glove_gensim[word]\n","        else:\n","            gensim_weight_matrix[index] = np.zeros(100)"]},{"cell_type":"markdown","metadata":{},"source":["## **MODEL**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow\n","from tensorflow.keras.models import Sequential \n","from tensorflow.keras.layers import Dense, LSTM, Embedding,Bidirectional,SimpleRNN \n","\n","# from tensorflow.compat.v1.keras.layers import CuDNNRNN\n","from tensorflow.keras.layers import Dropout, Flatten\n","from tensorflow.keras.optimizers import Adam"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#Splitting the data into training and testing\n","from sklearn.model_selection import train_test_split\n","y=pd.get_dummies(df['class'])\n","X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 123)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import matplotlib.pyplot as plt\n","\n","X_reshaped = X.reshape(X.shape[0], X.shape[1], 1)\n","\n","# Splitting the data into training and testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n","\n","# Define model architectures\n","EMBEDDING_DIM = 100\n","\n","def create_bilstm_rnn_model(input_shape, num_classes):\n","    model = Sequential([\n","        Embedding(input_dim=num_words, \n","                    output_dim=EMBEDDING_DIM, \n","                    input_length=X.shape[1], weights=[gensim_weight_matrix], \n","                    trainable=False),\n","        Dropout(0.275),\n","        Bidirectional(LSTM(units=350, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(units=450, return_sequences=True)),\n","        Dropout(0.225),\n","        Bidirectional(LSTM(units=500, return_sequences=True)),\n","        Dropout(0.45),\n","        SimpleRNN(125, return_sequences=True),\n","        Dropout(0.425),\n","        SimpleRNN(150, return_sequences=False),\n","        Dense(2, activation='softmax')\n","    ])\n","    return model\n","\n","def create_bilstm_model(input_shape, num_classes):\n","    model = Sequential([\n","        Embedding(input_dim=num_words, \n","                    output_dim=EMBEDDING_DIM, \n","                    input_length=X.shape[1], weights=[gensim_weight_matrix], \n","                    trainable=False),\n","        Dropout(0.275),\n","        Bidirectional(LSTM(units=350, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(units=450, return_sequences=True)),\n","        Dropout(0.225),\n","        Bidirectional(LSTM(units=500, return_sequences=False)),\n","        Dropout(0.45),\n","        Flatten(),\n","        Dense(2, activation='softmax')\n","    ])\n","    return model\n","\n","def create_lstm_model(input_shape, num_classes):\n","    model = Sequential([\n","        Embedding(input_dim=num_words, \n","                    output_dim=EMBEDDING_DIM, \n","                    input_length=X.shape[1], weights=[gensim_weight_matrix], \n","                    trainable=False),\n","        Dropout(0.275),\n","        LSTM(units=350, return_sequences=True),\n","        Dropout(0.25),\n","        LSTM(units=450, return_sequences=True),\n","        Dropout(0.225),\n","        LSTM(units=500, return_sequences=False),\n","        Dropout(0.45),\n","        Flatten(),\n","        Dense(2, activation='softmax')\n","    ])\n","    return model\n","\n","# Compile models\n","num_classes = 2\n","bilstm_rnn_model = create_bilstm_rnn_model(X.shape[1], num_classes)\n","bilstm_model = create_bilstm_model(X.shape[1], num_classes)\n","lstm_model = create_lstm_model(X.shape[1], num_classes)\n","\n","# Define callbacks\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n","mc = ModelCheckpoint('/kaggle/working/models/best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n","\n","# Train and evaluate models\n","models = {'Bi-LSTM-RNN': bilstm_rnn_model, 'Bi-LSTM': bilstm_model, 'LSTM': lstm_model}\n","evaluations = {}\n","histories = {}\n","\n","for name, model in models.items():\n","    print(f\"Training {name} Model...\")\n","    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n","    history = model.fit(X_train, y_train, \n","                        epochs=1, batch_size=128, \n","                        validation_data=(X_test, y_test),\n","                        verbose=1, callbacks=[mc])\n","    \n","    # Save model\n","    model.save(f'save_model_{name}.h5')\n","    \n","    # Save history\n","    histories[name] = history.history\n","    \n","    # Evaluate model\n","    evaluation = model.evaluate(X_test, y_test)\n","    evaluations[name] = evaluation\n","\n","# Print evaluation results\n","for name, evaluation in evaluations.items():\n","    print(f\"{name} Model Evaluation Loss:\", evaluation[0])\n","    print(f\"{name} Model Evaluation Accuracy:\", evaluation[1])\n","\n","# Plot training curves\n","for name, history in histories.items():\n","    plt.plot(history['accuracy'], label=f'{name} Train Accuracy')\n","    plt.plot(history['val_accuracy'], label=f'{name} Validation Accuracy')\n","\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","# Model Comparison Table\n","results = {\n","    'Model': list(evaluations.keys()),\n","    'Loss': [evaluation[0] for evaluation in evaluations.values()],\n","    'Accuracy': [evaluation[1] for evaluation in evaluations.values()]\n","}\n","\n","results_df = pd.DataFrame(results)\n","print(results_df)\n","\n","# Calculate precision, recall, and F1-score for each model\n","precision = {}\n","recall = {}\n","f1 = {}\n","\n","for name, model in models.items():\n","    y_pred = model.predict(X_test)\n","    y_pred = np.argmax(y_pred, axis=1)\n","    precision[name] = precision_score(np.argmax(y_test.to_numpy(), axis=1), y_pred)\n","    recall[name] = recall_score(np.argmax(y_test.to_numpy(), axis=1), y_pred)\n","    f1[name] = f1_score(np.argmax(y_test.to_numpy(), axis=1), y_pred)\n","\n","# Print precision, recall, and F1-score for each model\n","for name in models.keys():\n","    print(f\"{name} Precision:\", precision[name])\n","    print(f\"{name} Recall:\", recall[name])\n","    print(f\"{name} F1 Score:\", f1[name])\n","\n","# Create DataFrame for precision, recall, and F1-score\n","results_metrics = {\n","    'Model': list(models.keys()),\n","    'Precision': [precision[name] for name in models.keys()],\n","    'Recall': [recall[name] for name in models.keys()],\n","    'F1 Score': [f1[name] for name in models.keys()]\n","}\n","\n","results_metrics_df = pd.DataFrame(results_metrics)\n","\n","# Style the DataFrame for better visualization\n","styled_results_metrics_df = results_metrics_df.style.background_gradient(cmap='viridis', subset=['Precision', 'Recall', 'F1 Score'])\n","styled_results_metrics_df = styled_results_metrics_df.set_table_styles([{\n","    'selector': 'th',\n","    'props': [('font-size', '16px')]\n","}, {\n","    'selector': 'td',\n","    'props': [('font-size', '16px')]\n","}])\n","styled_results_metrics_df = styled_results_metrics_df.set_properties(**{'width': '300px', 'text-align': 'center'})\n","styled_results_metrics_df\n"]},{"cell_type":"markdown","metadata":{},"source":["# **ATTENTION MODEL**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer, Embedding, LSTM, Bidirectional, SimpleRNN, Dense, Dropout\n","from tensorflow.keras import Sequential\n","\n","class BahdanauAttention(Layer):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = Dense(units)\n","        self.W2 = Dense(units)\n","        self.V = Dense(1)\n","\n","    def call(self, inputs):\n","        features, hidden = inputs\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis)))\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        context_vector = attention_weights * features\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        return context_vector, attention_weights\n","\n","def create_bilstm_rnn_attention_model(input_shape, num_classes):\n","    model = Sequential([\n","        Embedding(input_dim=num_words, \n","                    output_dim=EMBEDDING_DIM, \n","                    input_length=X.shape[1], weights=[gensim_weight_matrix], \n","                    trainable=False),\n","        Dropout(0.275),\n","        Bidirectional(LSTM(units=350, return_sequences=True)),\n","        Dropout(0.25),\n","        Bidirectional(LSTM(units=450, return_sequences=True)),\n","        Dropout(0.225),\n","        Bidirectional(LSTM(units=500, return_sequences=True)),\n","        Dropout(0.45),\n","        SimpleRNN(125, return_sequences=True),\n","        Dropout(0.425),\n","        SimpleRNN(150, return_sequences=True),\n","        BahdanauAttention(64),  # Ensure that the inputs to this layer are in the correct format\n","        Dense(2, activation='softmax')\n","    ])\n","    return model\n","\n","\n","# Assuming you have the necessary data (X_train, y_train, X_test, y_test) and variables (num_words, EMBEDDING_DIM, gensim_weight_matrix) defined\n","\n","num_classes = 2\n","input_shape = X_train.shape[1:]\n","num_words = num_words  # Define this based on your tokenizer\n","embedding_dim = 100  # Define this based on your embedding dimension\n","weights_matrix = gensim_weight_matrix  # Assuming you have the weights matrix\n","\n","\n","# Create the model\n","attention_model = create_bilstm_rnn_attention_model(X.shape[1], num_classes)\n","\n","# Compile the model\n","attention_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","history = attention_model.fit(X_train, y_train, epochs=1, batch_size=128, validation_data=(X_test, y_test))\n","\n","# After training, you can obtain the attention weights for a given input by using model.predict() and accessing the attention weights layer output\n","# For example:\n","attention_weights = attention_model.predict(X_test)[1]  # Assuming attention layer is the second layer after Embedding\n","# You can now analyze these attention weights to identify influential words\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","# Get predictions and attention weights for test data\n","predictions = attention_model.predict(X_test)\n","\n","attention_weights = predictions[1]  # Assuming attention layer is the second layer after Embedding\n","\n","# Convert attention weights to numpy array\n","attention_weights_np = np.array(attention_weights)\n","\n","print(\"Shape of attention weights array:\", attention_weights_np)\n","\n","# Define a function to visualize influential words\n","def visualize_influential_words(input_sequence, attention_weights):\n","    # Get the index of the most influential word\n","    influential_word_index = np.argmax(attention_weights)\n","    # Get the corresponding word from the input sequence\n","    influential_word = input_sequence[influential_word_index]\n","    return influential_word\n","\n","# Choose a random sample index from the test data\n","sample_index = np.random.randint(0, len(X_test))\n","\n","# Get the input sequence and its corresponding attention weights\n","input_sequence = X_test[sample_index]\n","attention_weights_sample = attention_weights_np[sample_index]\n","\n","\n","# Visualize influential words for the selected sample\n","influential_word = visualize_influential_words(input_sequence, attention_weights_sample)\n","print(\"Influential Word:\", influential_word)\n"]},{"cell_type":"markdown","metadata":{},"source":["# **LOAD MODEL**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","\n","# Load the saved models\n","best_bilstm_rnn_model = load_model('save_model_Bi-LSTM-RNN.h5')\n","best_bilstm_model = load_model('save_model_Bi-LSTM.h5')\n","best_lstm_model = load_model('save_model_LSTM.h5')\n","\n","# Example input post\n","input_post = \"i want to die and want to play basketball\"\n","\n","# Function to highlight influential words\n","def highlight_words(text, influential_words):\n","    highlighted_text = text\n","    for word in influential_words:\n","        highlighted_text = highlighted_text.replace(word, f'<span style=\"background-color: yellow;\">{word}</span>')\n","    return highlighted_text\n","\n","# Tokenize the input post\n","tokenized_input_post = tokenizer.texts_to_sequences([input_post]) \n","\n","# Define maxlen (maximum length of sequences after padding)\n","maxlen = 150\n","\n","# Pad the sequences to ensure uniform length\n","padded_input_post = pad_sequences(tokenized_input_post, maxlen=maxlen, padding='post')\n","\n","# Predict the class of the input post using each model\n","predictions_bilstm_rnn = best_bilstm_rnn_model.predict(padded_input_post)\n","predictions_bilstm = best_bilstm_model.predict(padded_input_post)\n","predictions_lstm = best_lstm_model.predict(padded_input_post)\n","\n","# Function to analyze predictions and highlight influential words\n","def analyze_and_highlight(predictions, input_post):\n","    # For this simple example, let's assume the influential words are those with the highest probability\n","    influential_words = [input_post.split()[i] for i in np.argsort(predictions[0])[-3:]]\n","    highlighted_post = highlight_words(input_post, influential_words)\n","    return highlighted_post\n","\n","# Analyze predictions and highlight influential words for each model\n","highlighted_post_bilstm_rnn = analyze_and_highlight(predictions_bilstm_rnn, input_post)\n","highlighted_post_bilstm = analyze_and_highlight(predictions_bilstm, input_post)\n","highlighted_post_lstm = analyze_and_highlight(predictions_lstm, input_post)\n","\n","from IPython.display import HTML\n","\n","# Function to display HTML content\n","def display_html(html_content):\n","    display(HTML(html_content))\n","\n","# Display the highlighted posts\n","print(\"Highlighted Post - Bi-LSTM-RNN:\")\n","display_html(highlighted_post_bilstm_rnn)\n","\n","print(\"Highlighted Post - Bi-LSTM:\")\n","display_html(highlighted_post_bilstm)\n","\n","print(\"Highlighted Post - LSTM:\")\n","display_html(highlighted_post_lstm)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","\n","class PostHighlighter:\n","    def __init__(self):\n","        self.best_bilstm_rnn_model = None\n","        self.best_bilstm_model = None\n","        self.best_lstm_model = None\n","        self.class_mapping = {0: 'non-suicide', 1: 'suicide'}  # Mapping of numerical labels to class names\n","\n","    def load_models(self, bilstm_rnn_model_path, bilstm_model_path, lstm_model_path):\n","        self.best_bilstm_rnn_model = load_model(bilstm_rnn_model_path)\n","        self.best_bilstm_model = load_model(bilstm_model_path)\n","        self.best_lstm_model = load_model(lstm_model_path)\n","\n","    def highlight_words(self, text, influential_words):\n","        highlighted_text = text\n","        for word in influential_words:\n","            highlighted_text = highlighted_text.replace(word, f'<span style=\"background-color: yellow;\">{word}</span>')\n","        return highlighted_text\n","\n","    def classify_post(self, input_post):\n","        if not all([self.best_bilstm_rnn_model, self.best_bilstm_model, self.best_lstm_model]):\n","            print(\"Models not loaded. Please load the models first.\")\n","            return\n","        \n","        # Tokenize the input post\n","        tokenized_input_post = tokenizer.texts_to_sequences([input_post])\n","\n","        # Pad the sequences to ensure uniform length\n","        maxlen = 150\n","        padded_input_post = pad_sequences(tokenized_input_post, maxlen=maxlen, padding='post')\n","\n","        # Predict the class of the input post using each model\n","        predictions_bilstm_rnn = self.best_bilstm_rnn_model.predict(padded_input_post)\n","        predictions_bilstm = self.best_bilstm_model.predict(padded_input_post)\n","        predictions_lstm = self.best_lstm_model.predict(padded_input_post)\n","\n","        # Classify the post based on the model with the highest confidence\n","        model_names = ['Bi-LSTM-RNN', 'Bi-LSTM', 'LSTM']\n","        predicted_labels = []\n","        highlighted_posts = []\n","        for name, prediction in zip(model_names, [predictions_bilstm_rnn, predictions_bilstm, predictions_lstm]):\n","            predicted_label = np.argmax(prediction)\n","            predicted_class = self.class_mapping[predicted_label]  # Map numerical label to class name\n","            influential_words = [input_post.split()[i] for i in np.argsort(prediction[0])[-3:]]\n","            highlighted_post = self.highlight_words(input_post, influential_words)\n","            predicted_labels.append((name, predicted_class))  # Use predicted class instead of numerical label\n","            highlighted_posts.append((name, highlighted_post))\n","\n","        return predicted_labels, highlighted_posts\n","\n","# Example usage:\n","post_highlighter = PostHighlighter()\n","\n","# Load the trained models\n","post_highlighter.load_models('save_model_Bi-LSTM-RNN.h5', 'save_model_Bi-LSTM.h5', 'save_model_LSTM.h5')\n","\n","# Define an input post\n","input_post = \"let not play football\"\n","\n","from IPython.display import HTML\n","\n","# Classify the input post and highlight influential words\n","predicted_labels, highlighted_posts = post_highlighter.classify_post(input_post)\n","\n","# Display the highlighted posts\n","print(\"Predicted Labels:\", predicted_labels)\n","for name, highlighted_post in highlighted_posts:\n","    print(f\"{name}:\")\n","    display_html(highlighted_post)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1835,"sourceId":3176,"sourceType":"datasetVersion"},{"datasetId":2568,"sourceId":4304,"sourceType":"datasetVersion"},{"datasetId":213609,"sourceId":464671,"sourceType":"datasetVersion"},{"datasetId":1075326,"sourceId":2250642,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
